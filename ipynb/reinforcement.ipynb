{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# For Environment\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "# For RL\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnNoModelImprovement\n",
    "\n",
    "# Helpers\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing and Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data\n",
    "data = pd.read_csv('../dataset/raw_data.csv')\n",
    "\n",
    "# Removing Duplicates\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Sorting\n",
    "data.sort_values(by='project_id', ascending=False, inplace=True)\n",
    "data.sort_values(by=['project_id', 'period', 'sn'], inplace=True, ascending=False)\n",
    "\n",
    "# Backup\n",
    "data.to_csv('../dataset/data.csv', index=False)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spilt period into year, month, day\n",
    "data['year'] = data['period'].apply(lambda x: int(str(x)[:4])-1)\n",
    "data['month'] = data['period'].apply(lambda x: int(str(x)[4:6])-1)\n",
    "data['day'] = data['period'].apply(lambda x: int(str(x)[6:])-1)\n",
    "\n",
    "# Replace year with unique values from 0 to INF\n",
    "data['year'] = data['year'].astype('category').cat.codes\n",
    "\n",
    "# Dropping period\n",
    "data.drop('period', axis=1, inplace=True)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{data['year'].unique()}, {data['month'].unique()}, {data['day'].unique()}, {data['project_id'].unique()}, {data['target_colour'].unique()}, {data['target_number'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRADES_PER_EPISODE = 20\n",
    "NO_OF_STEPS = int(2e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_states() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns n continues states for the environment.\n",
    "        \n",
    "    Returns:\n",
    "        states (list): List of n continues states\n",
    "    \"\"\"\n",
    "    states = []\n",
    "    \n",
    "    # Get a random sample\n",
    "    first_val = data.sample()\n",
    "    \n",
    "    # find the index of the first_val in the data\n",
    "    index = first_val.index[0]\n",
    "    \n",
    "    # Convert it into a list and Append the first_val to states\n",
    "    states.append(first_val.values.tolist()[0])\n",
    "    \n",
    "    for state in range(TRADES_PER_EPISODE+1):\n",
    "        # Append the next_val to states\n",
    "        index += 1\n",
    "        \n",
    "        # Check whether the project_id is the same as the first_val and the index is less than the length of the data\n",
    "        if index < len(data) and data.iloc[index]['project_id'] == first_val['project_id'].values[0]:\n",
    "            states.append(data.iloc[index].values.tolist())\n",
    "    \n",
    "    states.pop(0)\n",
    "    # Check whether the length of the states is equal to the TRADES_PER_EPISODE\n",
    "    if len(states) != TRADES_PER_EPISODE+1:\n",
    "        # If not, recursively call the function\n",
    "        return get_states()\n",
    "        \n",
    "    return pd.DataFrame(states, columns=data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Site Environment\n",
    "class SiteEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    A custom environment for trading sites based on colour and number.\n",
    "    \n",
    "    Attributes:\n",
    "        trade_count (int): The current trade index in the state dataframe.\n",
    "        action_space (gym.spaces.MultiDiscrete): The action space, consisting of 4 discrete values: multiplier, bet_on, colour and number.\n",
    "        state (pandas.DataFrame): The state dataframe, containing the year, month, day, sn, project_id, target_colour, and target_number for each trade.\n",
    "        observation_space (gym.spaces.Box): The observation space, consisting of a Box of 5 discrete values: year, month, day, sn ( serial number ) and project_id.\n",
    "        length (int): The maximum number of trades per episode.\n",
    "    \"\"\"\n",
    "    def __init__(self, states: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Initialize the environment with the given state dataframe.\n",
    "        \n",
    "        Args:\n",
    "            states (pandas.DataFrame): The state year, month, day, sn, project_id, target_colour, and target_number for each trade.\n",
    "        \"\"\"\n",
    "        self.trade_count = 0\n",
    "        \n",
    "        # To punish the agent for repeating the same action\n",
    "        self.last_three_actions = []\n",
    "        \n",
    "        # To reward the agent for 3 consecutive correct actions\n",
    "        self.last_three_rewards = []\n",
    "        \n",
    "        # Define the action space as a multi-discrete space of two values: colour and number\n",
    "        # multiplier = 0 - 4 ( Show how strongly the agent feels about the trade )\n",
    "        # bet_on = 0 - 2 ( 0: colour, 1: number, 2: both )\n",
    "        # colour = 0: red, 1: green\n",
    "        # number = 0 - 9\n",
    "        self.action_space = spaces.MultiDiscrete([5, 3, 2, 10])\n",
    "        \n",
    "        # Store the state dataframe as an attribute\n",
    "        self.state = states\n",
    "        \n",
    "        # Define the observation space as a Box of 5 discrete values: year, month, day, sn ( serial number ) and project_id.\n",
    "        # year = 1\n",
    "        # month = 1 - 12\n",
    "        # day = 1 - 31\n",
    "        # sn = 0 - 999\n",
    "        # project_id = 0: Parity, 1: Sapre, 2: Bcone, 3: Emerd\n",
    "        self.observation_space = spaces.Box(low=np.array([1, 1, 1, 0, 0]), high=np.array([1, 12, 31, 999, 3]), dtype=np.int32)\n",
    "        \n",
    "        # Define the length as the number of trades per episode\n",
    "        self.length = TRADES_PER_EPISODE\n",
    "    \n",
    "    def step(self, action: tuple[int, int]) -> tuple:\n",
    "        \"\"\"\n",
    "        Run one timestep of the environment's dynamics.\n",
    "        \n",
    "        Args:\n",
    "            action (gym.spaces.MultiDiscrete): The action space, consisting of 4 discrete values: multiplier, bet_on, colour and number.\n",
    "            \n",
    "        Returns:\n",
    "            observation (gym.spaces.Box): The next observation, consisting of a dictionary of 5 values: year, month, day, sn ( serial number ) and project_id.\n",
    "            reward (int): The amount of reward returned as a result of taking the action.\n",
    "            terminated (bool): Whether the episode has ended, either because the trade count has reached the length, or the action is invalid.\n",
    "            truncated (bool): Whether the episode was truncated. ( For now no truncation )\n",
    "            info (dict): An empty dictionary, for compatibility with the gym interface.\n",
    "        \"\"\"\n",
    "        # Check if the action is valid\n",
    "        if not self.action_space.contains(action):\n",
    "            # Raise an exception if the action is invalid\n",
    "            raise ValueError(f\"Invalid action: {action}\")\n",
    "        \n",
    "        # Get the current state from the state dataframe\n",
    "        state = self.state.iloc[self.trade_count]\n",
    "        \n",
    "        reward = 0\n",
    "        # Check on_bet\n",
    "        if action[1] == 0:\n",
    "            # Bet on colour\n",
    "            if action[2] == state['target_colour']:\n",
    "                reward += (1 * (action[0] + 1))\n",
    "            else:\n",
    "                reward -= (1 * (action[0] + 1))\n",
    "        elif action[1] == 1:\n",
    "            # Bet on number\n",
    "            if action[3] == state['target_number']:\n",
    "                reward += (3 * (action[0] + 1))\n",
    "            else:\n",
    "                reward -= (1 * (action[0] + 1))\n",
    "        else:\n",
    "            # Bet on both\n",
    "            if action[2] == state['target_colour'] and action[3] == state['target_number']:\n",
    "                reward += (4 * (action[0] + 1))\n",
    "            else:\n",
    "                reward -= (2 * (action[0] + 1))\n",
    "        \n",
    "        # Reward for 3 consecutive correct actions on bet on both\n",
    "        # Append the current reward to the last three rewards\n",
    "        self.last_three_rewards.append([action[1], reward])\n",
    "        \n",
    "        # If more than three rewards have been appended, remove the first reward\n",
    "        if len(self.last_three_rewards) > 3:\n",
    "            self.last_three_rewards.pop(0)\n",
    "            \n",
    "        # Check if the last three rewards total >= 12\n",
    "        # If so, reward the agent\n",
    "        if len(self.last_three_rewards) == 3 and (self.last_three_rewards[0][1] + self.last_three_rewards[1][1] + self.last_three_rewards[2][1]) >= 12:\n",
    "            reward += 50\n",
    "            # print(\"Reward +10\")\n",
    "        elif len(self.last_three_rewards) == 3 and (self.last_three_rewards[0][1] + self.last_three_rewards[1][1] + self.last_three_rewards[2][1]) <= -6:\n",
    "            reward -= 50\n",
    "            # print(\"Punishment -20\")\n",
    "        \n",
    "        # Punishment for repeating the same action\n",
    "        # Append the current action's colour and number to the last_three_actions\n",
    "        self.last_three_actions.append(action[2:])\n",
    "        \n",
    "        # If more than three actions have been appended, remove the first action\n",
    "        if len(self.last_three_actions) > 3:\n",
    "            self.last_three_actions.pop(0)\n",
    "            \n",
    "        # Check if the last three actions are the same\n",
    "        # If so, punish the agent\n",
    "        if len(self.last_three_actions) == 3 and len(set(tuple(a) for a in self.last_three_actions)) == 1:\n",
    "            # print(\"Punishment -500\")\n",
    "            reward -= 500\n",
    "        \n",
    "        # Increment the trade count\n",
    "        self.trade_count += 1\n",
    "        \n",
    "        # Get the next observation from the state dataframe\n",
    "        observation = self.state.iloc[self.trade_count][['year', 'month', 'day', 'sn', 'project_id']].to_list()\n",
    "        \n",
    "        # Set the terminated flag to False, unless the trade count has reached the length\n",
    "        terminated = self.trade_count >= self.length\n",
    "        \n",
    "        # No truncation\n",
    "        truncated = False\n",
    "        \n",
    "        # Return an empty info dictionary, for compatibility with the gym interface\n",
    "        info = {}\n",
    "            \n",
    "        return observation, reward, terminated, truncated, info \n",
    "    \n",
    "    def render(self):\n",
    "        # This method is not implemented, as this environment does not have a visual representation\n",
    "        pass\n",
    "    \n",
    "    def reset(self, states=get_states(), seed=None):\n",
    "        \"\"\"\n",
    "        Reset the environment to an initial state and return the initial observation.\n",
    "        \n",
    "        Args:\n",
    "            states (pandas.DataFrame): The state dataframe, containing the period, count_of_the_day, project_id, target_colour, and target_number for each trade.\n",
    "            seed (int): The seed for the random number generator.\n",
    "            \n",
    "        Returns:\n",
    "            observation (gym.spaces.Box): The initial observation, consisting of a dictionary of 5 values: year, month, day, sn ( serial number ) and project_id.\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "        \n",
    "        # Reset the trade count to zero\n",
    "        self.trade_count = 0\n",
    "        \n",
    "        # Reset the last three actions to an empty list\n",
    "        self.last_three_actions = []\n",
    "        \n",
    "        # Reset the last three rewards to an empty list\n",
    "        self.last_three_rewards = []\n",
    "        \n",
    "        # Reset the state dataframe with the given states\n",
    "        self.state = states\n",
    "        \n",
    "        # Get the initial observation from the state dataframe\n",
    "        observation = self.state.iloc[self.trade_count][['year', 'month', 'day', 'sn', 'project_id']].to_list()\n",
    "        \n",
    "        # Reset the length to the number of trades per episode\n",
    "        self.length = TRADES_PER_EPISODE\n",
    "        \n",
    "        # Create an empty info dictionary\n",
    "        info = {}\n",
    "        \n",
    "        return observation, info\n",
    "    \n",
    "    def close(self):\n",
    "        # This method is not implemented, as this environment does not have any resources to release\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the Environment\n",
    "\n",
    "test_episodes = 3\n",
    "for episode in range(test_episodes):\n",
    "    env = SiteEnv(get_states())\n",
    "    state = env.reset(get_states())\n",
    "    terminated = False\n",
    "    score = 0\n",
    "    \n",
    "    while not terminated:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        score += reward\n",
    "        print(f\"Reward: {reward}, Done: {terminated}\")\n",
    "        \n",
    "    print('--------------------------')  \n",
    "    print(f\"Episode: {episode}, Score: {score}\")\n",
    "    print('--------------------------')\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Environment\n",
    "\n",
    "log_path = '../logs/'\n",
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Setting up the Callbacks\n",
    "stop_training_callback = StopTrainingOnNoModelImprovement(max_no_improvement_evals=10, min_evals=30, verbose=1)\n",
    "eval_callback = EvalCallback(env, eval_freq=10000, callback_after_eval=stop_training_callback, best_model_save_path='../models/', verbose=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=NO_OF_STEPS, callback=eval_callback)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Loading the logs\n",
    "# !tensorboard --logdir ../logs/"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the Model\n",
    "\n",
    "save_path = '../models/'\n",
    "model.save(save_path + 'PPO_Mlp_RL_v0.1.2-beta')\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '../models/'\n",
    "model = PPO.load(save_path + 'PPO_Mlp_RL_v0.1.2-beta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the Model\n",
    "\n",
    "evaluate_policy(model, env, n_eval_episodes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_observation(period: str, project_id: int) -> list:\n",
    "    \"\"\"\n",
    "    Make an observation from the given period and project_id.\n",
    "    \n",
    "    Args:\n",
    "        period (int): The period of the trade.\n",
    "        project_id (int): The project_id of the trade.\n",
    "        \n",
    "    Returns:\n",
    "        observation (gym.spaces.Box): The observation, consisting of a dictionary of 5 values: year, month, day, sn ( serial number ) and project_id.\n",
    "    \"\"\"\n",
    "    # Get the year, month, day, and sn from the period\n",
    "    year = int(str(period)[:4])-1\n",
    "    month = int(str(period)[4:6])-1\n",
    "    day = int(str(period)[6:8])-1\n",
    "    sn = int(str(period)[8:])\n",
    "    \n",
    "    # Convert the year into a category\n",
    "    year = pd.Series(year).astype('category').cat.codes[0]\n",
    "    \n",
    "    # Return the observation\n",
    "    return [year, month, day, sn, project_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Predictions\n",
    "model.predict(make_observation('20231205164', 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
